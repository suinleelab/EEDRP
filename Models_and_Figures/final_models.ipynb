{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "from processing_helper_functions import final_processing\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to choose which downsampling technique, time series encoding technique, and other settings to use\n",
    "\n",
    "# Which year of past data you would like to train on. One of: 0, 1, 2 where 0 is most recent and 2 is least recent\n",
    "# If you want to train on more than one year, enter the least recent year to include\n",
    "years_data = 0\n",
    "# Set to True if you want to only train on 1 year of data\n",
    "# Set to False to train on all data through years variable above\n",
    "only_one = True\n",
    "# Which time encoding technique should be used\n",
    "# One of: 'all', 'slopes', 'all_ma', 'sma', 'ema'\n",
    "encode_method = 'all'\n",
    "# Which set of features should be used\n",
    "# One of: 'baseline_demographics_cols', 'baseline_mci_cols', 'baseline_demographics_withmci_cols', 'baseline_mmse30_cols',\n",
    "# 'baseline_linear_selected_features','simplified_cols_withapoe', 'simplified_cols_withoutapoe', 'all_cols'\n",
    "feature_set = 'simplified_cols_withapoe'\n",
    "# Which downsampling technique should be used\n",
    "# One of: 'randdownsample', 'matcheddownsample', 'train', 'weighted'\n",
    "sample_type = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_suffix = encode_method\n",
    "if encode_method in ['all','current']:\n",
    "    encode_suffix = str(years_data+1)\n",
    "if encode_method == 'ema':\n",
    "    encode_suffix = encode_method+str(half_life)\n",
    "if only_one:\n",
    "    save_suffix = '%s_%s_2yrprev_within3_singleyear%s'%(sample_type, feature_set, str(int(encode_suffix)-1))\n",
    "else:\n",
    "    save_suffix = '%s_%s_2yrprev_within3_yrsincluded%s'%(sample_type, feature_set, encode_suffix)\n",
    "\n",
    "file_suffix = \"2yrprev_within3\"\n",
    "with h5py.File(\"../DATA/PROCESSED/standardized_stacked_imputed/%s.h5\"%file_suffix, 'r') as hf:\n",
    "    ALL_SAMPLES = hf[\"samples\"][:]\n",
    "    ALL_FEATURES_TIME = hf[\"features\"][:]\n",
    "DATA = pd.read_csv(\"../DATA/PROCESSED/standardized/merged_kept_data_%s.csv\"%file_suffix, index_col=0)\n",
    "ALL_SAMPLES_df = pd.DataFrame(ALL_SAMPLES, columns=[\"projid\",\"fu_year\",\"onset_label_time\",\"onset_label_time_binary\"])\n",
    "\n",
    "constructed_data = final_processing(encode_method, years_data, only_one, feature_set, DATA.columns[6:], ALL_FEATURES_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT\n",
    "accuracy_save = []\n",
    "roc_auc_save = []\n",
    "pr_auc_save = []\n",
    "precision_save = []\n",
    "recall_save = []\n",
    "fpr_save = []\n",
    "tpr_save = []\n",
    "\n",
    "if sample_type in ['randdownsample','matcheddownsample']:\n",
    "    train_samples_df = pd.read_csv(\"../DATA/PROCESSED/split_projids/%s_train_%s.txt\"%(sample_type,file_suffix),index_col=0).drop_duplicates()\n",
    "    train_samples_df[\"keep\"]=1\n",
    "    data_train = pd.merge(DATA, train_samples_df, on=['projid','fu_year'], how='left')\n",
    "    train_idx = (data_train[\"keep\"]==1).values\n",
    "else:\n",
    "    train_idx = ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/train_%s.txt\"%(file_suffix))).values\n",
    "valid_idx = ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/test_%s.txt\"%(file_suffix))).values\n",
    "\n",
    "## Code to test effect of dropping missing rows and highly missing cols\n",
    "## To run: first scroll down to the \"Code to run JBHI revision experiments\" section \n",
    "##         and run the \"Code to test effect of dropping missing rows and highly missing cols\" cell.\n",
    "##         Then uncomment the next 3 lines and run this cell.\n",
    "#not_missing = ALL_SAMPLES_df.set_index(['projid','fu_year']).index.isin(pd.read_csv(\"../Data_Processing/JBHI_REV_IMPUTATION_EXPERIMENTS/to_keep_rows.csv\").set_index(['projid','fu_year']).index)\n",
    "#train_idx = np.logical_and(train_idx, not_missing)\n",
    "#valid_idx = np.logical_and(valid_idx, not_missing)\n",
    "\n",
    "## Code for between study (ROS vs MAP) validation\n",
    "## To run: first scroll down to the \"Code to run JBHI revision experiments\" section \n",
    "##         and run the \"Code for between study (ROS vs MAP) validation\" cell.\n",
    "##         Then uncomment the next 4 lines and run this cell.\n",
    "#train_exp = np.intersect1d(np.loadtxt(\"../DATA/PROCESSED/split_projids/train_%s.txt\"%(file_suffix)), MAP_ids)\n",
    "#valid_exp = np.intersect1d(np.loadtxt(\"../DATA/PROCESSED/split_projids/test_%s.txt\"%(file_suffix)), MAP_ids)\n",
    "#train_idx = ALL_SAMPLES_df[\"projid\"].isin(train_exp).values\n",
    "#valid_idx = ALL_SAMPLES_df[\"projid\"].isin(valid_exp).values\n",
    "    \n",
    "\n",
    "label_col = np.where(ALL_SAMPLES_df.columns.values=='onset_label_time_binary')[0][0]\n",
    "label_tr = ALL_SAMPLES[train_idx][:, label_col]\n",
    "label_val = ALL_SAMPLES[valid_idx][:, label_col] \n",
    "data_tr = constructed_data[train_idx]\n",
    "data_val = constructed_data[valid_idx]\n",
    "\n",
    "if sample_type == 'weighted':\n",
    "    weight_tr = label_tr.astype(float)\n",
    "    weight_tr[weight_tr==0] = np.sum(weight_tr)/(weight_tr.shape[0]-np.sum(weight_tr))\n",
    "    dmat_train = xgb.DMatrix(data_tr.fillna(-999.0), label=label_tr, missing=-999.0, weight=weight_tr)\n",
    "else:\n",
    "    dmat_train = xgb.DMatrix(data_tr.fillna(-999.0), label=label_tr, missing=-999.0)\n",
    "dmat_test = xgb.DMatrix(data_val.fillna(-999.0), label=label_val, missing=-999.0)\n",
    "num_round = 50\n",
    "param = {'silent':1, 'min_child_weight':0.25, 'eta':0.1, 'max_depth': 4, 'objective': 'binary:logistic', 'eval_metric': ['error','logloss']}\n",
    "model = xgb.train(param, dmat_train, num_boost_round=num_round, evals=[(dmat_train,'train'), (dmat_test,'eval')], early_stopping_rounds=5, verbose_eval=False)\n",
    "\n",
    "predr = model.predict(dmat_test, ntree_limit=model.best_iteration)\n",
    "predr_df = ALL_SAMPLES_df[[\"projid\",\"fu_year\"]][valid_idx]\n",
    "predr_df['predr'] = predr\n",
    "if feature_set == 'baseline_mci_cols':\n",
    "    predr = data_val['mci']\n",
    "accuracy_save.append(metrics.accuracy_score(label_val,np.round(predr)))\n",
    "fpr, tpr, _ = metrics.roc_curve(label_val,predr)\n",
    "fpr_save = fpr\n",
    "tpr_save = tpr\n",
    "roc_auc_save.append(metrics.auc(fpr,tpr))\n",
    "precision, recall, _ = metrics.precision_recall_curve(label_val,predr)\n",
    "precision_save.append(precision)\n",
    "recall_save.append(recall)\n",
    "pr_auc_save.append(metrics.auc(recall, precision))\n",
    "new_results = [accuracy_save,roc_auc_save,pr_auc_save,precision_save,recall_save,fpr_save,tpr_save]\n",
    "#print(new_results[0:3])\n",
    "#predr_df.to_csv('../results/xgb_preds_%s.csv'%(save_suffix))\n",
    "#pickle.dump(new_results, open('../results/xgb_%s.p'%(save_suffix), 'wb'))\n",
    "#pickle.dump(model, open('../results/xgb_%s.dat'%(save_suffix), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear\n",
    "accuracy_save = []\n",
    "roc_auc_save = []\n",
    "pr_auc_save = []\n",
    "precision_save = []\n",
    "recall_save = []\n",
    "fpr_save = []\n",
    "tpr_save = []\n",
    "\n",
    "if sample_type in ['randdownsample','matcheddownsample']:\n",
    "    train_samples_df = pd.read_csv(\"../DATA/PROCESSED/split_projids/%s_train_%s.txt\"%(sample_type,file_suffix),index_col=0).drop_duplicates()\n",
    "    train_samples_df[\"keep\"]=1\n",
    "    data_train = pd.merge(DATA, train_samples_df, on=['projid','fu_year'], how='left')\n",
    "    train_idx = (data_train[\"keep\"]==1).values\n",
    "else:\n",
    "    train_idx =ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/train_%s.txt\"%(file_suffix))).values\n",
    "valid_idx =ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/test_%s.txt\"%(file_suffix))).values\n",
    "\n",
    "label_col = np.where(ALL_SAMPLES_df.columns.values=='onset_label_time_binary')[0][0]\n",
    "label_tr = ALL_SAMPLES[train_idx][:, label_col]\n",
    "label_val =ALL_SAMPLES[valid_idx][:, label_col] \n",
    "data_tr = constructed_data[train_idx]\n",
    "data_val = constructed_data[valid_idx]\n",
    "\n",
    "if sample_type == 'weighted':\n",
    "    weight_tr = label_tr.astype(float)\n",
    "    weight_tr_val = np.sum(weight_tr)/(weight_tr.shape[0]-np.sum(weight_tr))\n",
    "    clf = LogisticRegression(class_weight={0: weight_tr_val, 1: 1})\n",
    "else:\n",
    "    clf = LogisticRegression()\n",
    "clf.fit(data_tr, label_tr)\n",
    "\n",
    "predr = clf.predict_proba(data_val)[:,1]\n",
    "predr_df = ALL_SAMPLES_df[[\"projid\",\"fu_year\"]][valid_idx]\n",
    "predr_df['predr'] = predr\n",
    "accuracy_save.append(clf.score(data_val, label_val))\n",
    "roc_auc_save.append(metrics.roc_auc_score(label_val, clf.decision_function(data_val)))\n",
    "pr_auc_save.append(metrics.average_precision_score(label_val, clf.decision_function(data_val)))   \n",
    "fpr_save, tpr_save, _ = metrics.roc_curve(label_val, clf.decision_function(data_val))\n",
    "precision_save, recall_save, _ = metrics.precision_recall_curve(label_val, clf.decision_function(data_val))\n",
    "\n",
    "new_results = [accuracy_save,roc_auc_save,pr_auc_save,precision_save,recall_save,fpr_save,tpr_save]\n",
    "#print(new_results[0:3])\n",
    "#predr_df.to_csv('../results/xgb_preds_%s.csv'%(save_suffix))\n",
    "#pickle.dump(new_results, open('../results/linear_%s.p'%(save_suffix), 'wb'))\n",
    "#pickle.dump(clf, open('../results/linear_%s.dat'%(save_suffix), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Code to run JBHI revision experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to test effect of dropping missing rows and highly missing cols\n",
    "## To run: first run this cell. Then scroll up to the Train XGB cell and \n",
    "##         uncomment the 3 lines underneath the \"Code to test effect of dropping missing rows and highly missing cols\" comment\n",
    "todrop_cols = np.loadtxt(\"../Data_Processing/JBHI_REV_IMPUTATION_EXPERIMENTS/to_drop_cols.txt\", dtype=str)\n",
    "for col in todrop_cols:\n",
    "    constructed_data.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for between study (ROS vs MAP) validation\n",
    "## To run: first run this cell. Then scroll up to the Train XGB cell and \n",
    "##         uncomment the 4 lines underneath the \"Code for between study (ROS vs MAP) validation\" comment\n",
    "DATA_dope = pd.read_csv(\"../DATA_dope/PROCESSED/standardized/merged_kept_data_new_%s.csv\"%file_suffix, index_col=0)\n",
    "ROSMAP_ids = DATA_dope[['projid','study']].drop_duplicates()\n",
    "ROS_ids = ROSMAP_ids['projid'].loc[ROSMAP_ids['study'] == 'ROS ']\n",
    "MAP_ids = ROSMAP_ids['projid'].loc[ROSMAP_ids['study'] == 'MAP ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstrap standard error intervals for accuracy, AUROC, and AUPRC metrics\n",
    "n_bootstraps = 1000\n",
    "rng_seed = len(label_val)  # control reproducibility\n",
    "bootstrapped_accuracy_scores = []\n",
    "bootstrapped_auroc_scores = []\n",
    "bootstrapped_auprc_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(predr), len(predr))\n",
    "\n",
    "    bootstrapped_accuracy_scores.append(metrics.accuracy_score(label_val[indices], np.round(predr[indices])))\n",
    "    fpr, tpr, _ = metrics.roc_curve(label_val[indices],predr[indices])\n",
    "    bootstrapped_auroc_scores.append(metrics.auc(fpr,tpr))\n",
    "    precision, recall, _ = metrics.precision_recall_curve(label_val[indices],predr[indices])\n",
    "    bootstrapped_auprc_scores.append(metrics.auc(recall, precision))\n",
    "print(sem(bootstrapped_accuracy_scores))\n",
    "print(sem(bootstrapped_auroc_scores))\n",
    "print(sem(bootstrapped_auprc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate confusion matrix\n",
    "matrix = metrics.confusion_matrix(label_val.astype(bool), np.where(predr > 0.5, 1, 0), labels=[0, 1])\n",
    "tn, fp, fn, tp = matrix.ravel()\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "print(matrix)\n",
    "print(sensitivity)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate IDI\n",
    "## Must have new_val calculated using correct model using predr previously in order to run\n",
    "#new_val = np.mean(predr[label_val==1.0])-np.mean(predr[label_val==0.0])\n",
    "old_val = np.mean(predr[label_val==1.0])-np.mean(predr[label_val==0.0])\n",
    "print(new_val - old_val)\n",
    "print((new_val - old_val)/old_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
